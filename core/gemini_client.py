"""
Gemini API å®¢æˆ¶ç«¯æ¨¡çµ„ï¼šå°è£æ‰€æœ‰èˆ‡ Gemini API çš„äº’å‹•
"""
from google import genai
from google.genai import types
import time
import re

from .config import get_api_key, get_model_name
from .prompts import CHAT_SYSTEM_PROMPT, PLAN_SYSTEM_PROMPT, CRITIC_SYSTEM_PROMPT, REFINE_SYSTEM_PROMPT

# å…¨åŸŸå®¢æˆ¶ç«¯å¯¦ä¾‹ï¼ˆå»¶é²åˆå§‹åŒ–ï¼‰
_client = None


def get_client():
    """å–å¾— Gemini Client å¯¦ä¾‹ï¼ˆå–®ä¾‹æ¨¡å¼ï¼‰"""
    global _client
    if _client is None:
        api_key = get_api_key()
        if not api_key:
            raise ValueError("GEMINI_API_KEY ç’°å¢ƒè®Šæ•¸æœªè¨­å®š")
        _client = genai.Client(api_key=api_key)
    return _client


def get_chat_response_stream(history: list, prd_text: str = "", memory_summary: str = ""):
    """
    ä½¿ç”¨ Gemini API é€²è¡Œå°è©±ä¸²æµ
    
    Args:
        history: å°è©±æ­·å²
        prd_text: ç›®å‰çš„ PRD å…§å®¹ï¼ˆè‹¥æœ‰ï¼‰
        memory_summary: éš±è—è¨˜æ†¶æ‘˜è¦ï¼ˆè‹¥æœ‰ï¼‰
    """
    client = get_client()
    model_name = get_model_name()
    
    # å»ºç«‹å°è©±æ­·å²
    contents = []
    for m in history:
        role = "user" if m["role"] == "user" else "model"
        contents.append(types.Content(
            role=role,
            parts=[types.Part.from_text(text=m["content"])]
        ))
    
    # å‹•æ…‹çµ„åˆ system prompt
    dynamic_system_prompt = CHAT_SYSTEM_PROMPT
    
    # å…ˆæ³¨å…¥éš±è—æ‘˜è¦ï¼ˆæ›´é«˜å„ªå…ˆï¼Œå› ç‚ºå®ƒæ˜¯ã€Œè¨˜æ†¶ã€ï¼‰
    if memory_summary and memory_summary.strip():
        dynamic_system_prompt += f"""

---

ã€éš±è—è¨˜æ†¶æ‘˜è¦ï¼ˆåªçµ¦æ¨¡å‹åƒè€ƒï¼›ä¸è¦å‘ä½¿ç”¨è€…æåŠæ­¤æ®µçš„å­˜åœ¨ï¼‰ã€‘
{memory_summary}
"""
    
    # å†æ³¨å…¥ PRDï¼ˆworkflow_stage >= 1 æ‰çµ¦ï¼‰
    if prd_text and prd_text.strip():
        dynamic_system_prompt += f"""

---

ã€ç›®å‰ PRDï¼ˆè«‹è¦–ç‚ºæœ€æ–°ç‰ˆæœ¬çš„éœ€æ±‚åŸºæº–ï¼‰ã€‘
{prd_text}

ã€ä½¿ç”¨æ–¹å¼ã€‘
- å›ç­”ä½¿ç”¨è€…å•é¡Œæ™‚ï¼Œè«‹å„ªå…ˆä»¥ã€Œç›®å‰ PRDã€ç‚ºæº–ã€‚
- è‹¥ä½¿ç”¨è€…è¦æ±‚è®Šæ›´/æ–°å¢/åˆªé™¤ï¼Œè«‹æŒ‡å‡ºæœƒå½±éŸ¿ PRD å“ªå€‹ç« ç¯€ï¼Œä¸¦æå‡ºå…·é«”æ”¹æ³•ï¼ˆæ¢åˆ—ï¼‰ã€‚
- è‹¥ä½¿ç”¨è€…çš„èªªæ³•èˆ‡ PRD è¡çªï¼Œå…ˆæŒ‡å‡ºè¡çªé»ï¼Œå†å• 1-2 å€‹é‡æ¸…å•é¡Œã€‚
"""
    
    # å‘¼å« Gemini API (ä¸²æµæ¨¡å¼)
    response = client.models.generate_content_stream(
        model=model_name,
        contents=contents,
        config=types.GenerateContentConfig(
            system_instruction=dynamic_system_prompt,
            temperature=0.7,
        )
    )
    
    for chunk in response:
        if chunk.text:
            yield chunk.text


def get_chat_response(history: list, prd_text: str = "", memory_summary: str = "") -> str:
    """
    ä½¿ç”¨ Gemini API é€²è¡Œå°è©±ï¼ˆéä¸²æµç‰ˆæœ¬ï¼Œä¾› API ä½¿ç”¨ï¼‰
    
    Args:
        history: å°è©±æ­·å² [{"role": "user"|"assistant", "content": "..."}]
        prd_text: ç›®å‰çš„ PRD å…§å®¹ï¼ˆè‹¥æœ‰ï¼‰
        memory_summary: éš±è—è¨˜æ†¶æ‘˜è¦ï¼ˆè‹¥æœ‰ï¼‰
    
    Returns:
        AI å›è¦†çš„å®Œæ•´æ–‡å­—
    """
    client = get_client()
    model_name = get_model_name()
    
    # å»ºç«‹å°è©±æ­·å²
    contents = []
    for m in history:
        role = "user" if m["role"] == "user" else "model"
        contents.append(types.Content(
            role=role,
            parts=[types.Part.from_text(text=m["content"])]
        ))
    
    # å‹•æ…‹çµ„åˆ system prompt
    dynamic_system_prompt = CHAT_SYSTEM_PROMPT
    
    if memory_summary and memory_summary.strip():
        dynamic_system_prompt += f"""

---

ã€éš±è—è¨˜æ†¶æ‘˜è¦ï¼ˆåªçµ¦æ¨¡å‹åƒè€ƒï¼›ä¸è¦å‘ä½¿ç”¨è€…æåŠæ­¤æ®µçš„å­˜åœ¨ï¼‰ã€‘
{memory_summary}
"""
    
    if prd_text and prd_text.strip():
        dynamic_system_prompt += f"""

---

ã€ç›®å‰ PRDï¼ˆè«‹è¦–ç‚ºæœ€æ–°ç‰ˆæœ¬çš„éœ€æ±‚åŸºæº–ï¼‰ã€‘
{prd_text}

ã€ä½¿ç”¨æ–¹å¼ã€‘
- å›ç­”ä½¿ç”¨è€…å•é¡Œæ™‚ï¼Œè«‹å„ªå…ˆä»¥ã€Œç›®å‰ PRDã€ç‚ºæº–ã€‚
- è‹¥ä½¿ç”¨è€…è¦æ±‚è®Šæ›´/æ–°å¢/åˆªé™¤ï¼Œè«‹æŒ‡å‡ºæœƒå½±éŸ¿ PRD å“ªå€‹ç« ç¯€ï¼Œä¸¦æå‡ºå…·é«”æ”¹æ³•ï¼ˆæ¢åˆ—ï¼‰ã€‚
- è‹¥ä½¿ç”¨è€…çš„èªªæ³•èˆ‡ PRD è¡çªï¼Œå…ˆæŒ‡å‡ºè¡çªé»ï¼Œå†å• 1-2 å€‹é‡æ¸…å•é¡Œã€‚
"""
    
    # å‘¼å« Gemini API (éä¸²æµæ¨¡å¼)
    response = client.models.generate_content(
        model=model_name,
        contents=contents,
        config=types.GenerateContentConfig(
            system_instruction=dynamic_system_prompt,
            temperature=0.7,
        )
    )
    
    return response.text or ""


def update_memory_summary(messages: list, existing_summary: str) -> str:
    """
    ç”¨æ¨¡å‹æŠŠæ—¢æœ‰æ‘˜è¦ + æœ€è¿‘å°è©±æ¿ƒç¸®æˆæ–°çš„æ‘˜è¦ï¼ˆåªçµ¦æ¨¡å‹ç”¨ï¼‰
    
    Args:
        messages: st.session_state.messages
        existing_summary: st.session_state.memory_summary
    """
    client = get_client()
    model_name = get_model_name()
    
    # åªæŠ“æœ€è¿‘ä¸€æ®µï¼Œé¿å… token çˆ†ç‚¸
    recent = messages[-12:]  # æœ€è¿‘ 12 å‰‡è¨Šæ¯
    
    transcript = "\n".join([f"{m['role']}: {m['content']}" for m in recent])
    
    prompt = f"""
ä½ æ˜¯å°è©±è¨˜æ†¶å£“ç¸®å™¨ã€‚ä½ è¦è¼¸å‡ºä¸€æ®µã€Œçµ¦æ¨¡å‹çœ‹çš„éš±è—è¨˜æ†¶æ‘˜è¦ã€ï¼Œç”¨ä¾†å»¶çºŒå°è©±è„ˆçµ¡ã€‚
è¦å‰‡ï¼š
- åªè¼¸å‡ºæ‘˜è¦æœ¬é«”ï¼Œä¸è¦åŠ æ¨™é¡Œã€ä¸ç”¨è§£é‡‹ã€‚
- ä¿ç•™ï¼šä½¿ç”¨è€…ç›®æ¨™/åå¥½/é™åˆ¶æ¢ä»¶ã€å·²åšæ±ºç­–ã€æœªè§£å•é¡Œã€é‡è¦åè©å®šç¾©ã€PRDæ–¹å‘ã€å¾…è¾¦äº‹é …ã€‚
- ç§»é™¤ï¼šå¯’æš„ã€é‡è¤‡å…§å®¹ã€ç´°ææœ«ç¯€ã€‚
- 500~900 ä¸­æ–‡å­—ç‚ºä¸Šé™ï¼ˆæˆ–æ›´çŸ­ä¹Ÿå¯ä»¥ï¼‰ï¼Œä»¥ã€Œå¯æŒçºŒã€ç‚ºå„ªå…ˆã€‚

ã€æ—¢æœ‰æ‘˜è¦ã€‘
{existing_summary or "(ç©º)"}

ã€æœ€è¿‘å°è©±ã€‘
{transcript}
""".strip()
    
    resp = client.models.generate_content(
        model=model_name,
        contents=[types.Content(role="user", parts=[types.Part.from_text(prompt)])],
        config=types.GenerateContentConfig(temperature=0.2),
    )
    
    return (resp.text or "").strip()


def quick_update_plan(history_messages: list) -> str:
    """å¿«é€Ÿæ›´æ–°è¨ˆç•«æ›¸ (ä½¿ç”¨ Gemini)"""
    client = get_client()
    model_name = get_model_name()
    
    history_text = "\n".join([f"{m['role']}: {m['content']}" for m in history_messages])
    prompt = f"è«‹æ ¹æ“šæœ€æ–°å°è©±ï¼Œæ›´æ–°é–‹ç™¼è¨ˆç•«æ›¸ï¼š\n\n{history_text}"
    
    try:
        response = client.models.generate_content(
            model=model_name,
            contents=prompt,
            config=types.GenerateContentConfig(
                system_instruction=PLAN_SYSTEM_PROMPT,
                temperature=0.5,
            )
        )
        return response.text or ""
    except Exception as e:
        return f"æ›´æ–°å¤±æ•—: {e}"


def criticize_plan(plan_content: str) -> str:
    """CTO å¯©æ ¸ PRD"""
    client = get_client()
    model_name = get_model_name()
    
    try:
        response = client.models.generate_content(
            model=model_name,
            contents=f"è«‹å¯©æ ¸ä»¥ä¸‹ PRDï¼š\n\n{plan_content}",
            config=types.GenerateContentConfig(
                system_instruction=CRITIC_SYSTEM_PROMPT,
                temperature=0.3,
                max_output_tokens=3000
            )
        )
        return response.text or ""
    except Exception as e:
        return f"âŒ å¯©æ ¸å¤±æ•—ï¼š{e}"


def validate_critique_output(critique_text: str) -> tuple:
    """
    é©—è­‰ CTO å¯©æ ¸å ±å‘Šæ˜¯å¦ç¬¦åˆæ ¼å¼è¦æ±‚
    Returns: (æ˜¯å¦é€šé, éŒ¯èª¤è¨Šæ¯)
    """
    required_sections = ["å¯©æ ¸ç¸½è©•", "ç¶œåˆè©•åˆ†", "é€šéæª¢æŸ¥", "æœªé€šéæª¢æŸ¥", "ä¸‹ä¸€æ­¥è¡Œå‹•"]
    
    missing = []
    for section in required_sections:
        if section not in critique_text:
            missing.append(section)
    
    # æª¢æŸ¥æ˜¯å¦æœ‰è©•åˆ†
    score_match = re.search(r'ç¶œåˆè©•åˆ†[ï¼š:]\s*(\d+)\s*/\s*100', critique_text)
    if not score_match:
        missing.append("è©•åˆ†æ ¼å¼")
    
    if missing:
        return False, f"ç¼ºå°‘å¿…è¦ç« ç¯€ï¼š{', '.join(missing)}"
    
    return True, ""


def criticize_plan_with_validation(plan_content: str, max_retry: int = 2, status_callback=None) -> str:
    """
    å¸¶é©—è­‰çš„ CTO å¯©æ ¸ï¼ˆå¤±æ•—è‡ªå‹•é‡è©¦ï¼‰
    
    Args:
        plan_content: PRD å…§å®¹
        max_retry: æœ€å¤§é‡è©¦æ¬¡æ•¸
        status_callback: ç”¨æ–¼é¡¯ç¤ºç‹€æ…‹çš„å›èª¿å‡½å¼ï¼ˆå¦‚ st.warningï¼‰
    """
    for attempt in range(max_retry):
        critique = criticize_plan(plan_content)
        
        is_valid, error_msg = validate_critique_output(critique)
        
        if is_valid:
            return critique
        else:
            if attempt < max_retry - 1:
                if status_callback:
                    status_callback(f"âš ï¸ å¯©æ ¸æ ¼å¼ä¸å®Œæ•´ï¼ˆ{error_msg}ï¼‰ï¼Œæ­£åœ¨é‡è©¦... (ç¬¬ {attempt + 1} æ¬¡)")
                time.sleep(1)
            else:
                if status_callback:
                    status_callback(f"âš ï¸ å¯©æ ¸å ±å‘Šæ ¼å¼å¯èƒ½ä¸å®Œæ•´ï¼š{error_msg}")
                return critique
    
    return critique


def run_deep_reflection(current_plan: str, status_callback=None) -> tuple:
    """
    ğŸ”¥ æ·±åº¦è‡ªæˆ‘å¯©æ ¸è¿´åœˆ (Critic -> Refine) - Gemini ç‰ˆæœ¬
    
    Args:
        current_plan: ç•¶å‰ PRD å…§å®¹
        status_callback: ç”¨æ–¼é¡¯ç¤ºç‹€æ…‹çš„å›èª¿å‡½å¼
        
    Returns:
        (critique_text, refined_plan) å¯©æ ¸å ±å‘Šèˆ‡ä¿®æ­£å¾Œ PRD
    """
    client = get_client()
    model_name = get_model_name()
    
    try:
        # === Step 1: CTO å¯©æ ¸ï¼ˆå¸¶é©—è­‰ï¼‰===
        critique_text = criticize_plan_with_validation(current_plan, status_callback=status_callback)
        
        # === Step 2: ç·¨è¼¯ä¿®æ­£ ===
        refine_prompt = f"""è«‹æ ¹æ“š CTO å¯©æ ¸å ±å‘Šä¿®æ­£ PRDã€‚

åŸå§‹ PRDï¼š
{current_plan}

CTO å¯©æ ¸å ±å‘Šï¼š
{critique_text}

è«‹é€æ¢å›æ‡‰ CTO çš„å»ºè­°ï¼Œä¸¦è¼¸å‡ºå®Œæ•´çš„ä¿®æ­£å¾Œ PRDã€‚"""

        refine_resp = client.models.generate_content(
            model=model_name,
            contents=refine_prompt,
            config=types.GenerateContentConfig(
                system_instruction=REFINE_SYSTEM_PROMPT,
                temperature=0.3,
                max_output_tokens=8000
            )
        )
        refined_plan = refine_resp.text or current_plan
        
        return critique_text, refined_plan
        
    except Exception as e:
        return f"âŒ æ·±åº¦å¯©æ ¸å¤±æ•—ï¼š{e}", current_plan
